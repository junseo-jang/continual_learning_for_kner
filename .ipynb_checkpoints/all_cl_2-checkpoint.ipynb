{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER'] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "#os.environ[\"CUDA_LAUNCH_BLOCKING\"] = '1'\n",
    "os.environ['WANDB_DISABLED'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jjs970612/anaconda3/envs/temp/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import re\n",
    "import random\n",
    "import extendNER_new_bioe_2 as ex #bio or bioe\n",
    "from transformers import ElectraForTokenClassification, AutoConfig, AutoTokenizer\n",
    "from transformers import AdamW\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset as DS\n",
    "from datasets import Dataset\n",
    "from transformers import DefaultDataCollator\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from seqeval.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.scheme import IOB1, IOB2, IOE1, IOE2, IOBES, BILOU, Entities, Prefix, Tag\n",
    "from seqeval.scheme import IOBES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(DS):\n",
    "    def __init__(self, dataset, tokenizer, label2id, max_length):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label2id = label2id\n",
    "        self.max_length = max_length\n",
    "        self.dataset = dataset\n",
    "        self.data = []\n",
    "        \n",
    "        for i in range(len(self.dataset['label'])):\n",
    "        #for i in range(3):\n",
    "            text = dataset['label'][i]\n",
    "            tagged_words = re.findall('<.*?:.*?>', text)\n",
    "            \n",
    "            word2ids = dict()\n",
    "            for tagged_word in tagged_words:\n",
    "                tag_splited = tagged_word.strip('<>').split(':')\n",
    "                tag = tag_splited[-1]\n",
    "                word = ':'.join(tag_splited[:-1])\n",
    "                    \n",
    "                word_tok = self.tokenizer.encode(word)[1:-1]\n",
    "                if word not in word2ids:\n",
    "\n",
    "                    label_id = [self.label2id['B-'+tag]]\n",
    "                    if len(word_tok) > 1 :\n",
    "                        label_id.extend([self.label2id['I-'+tag]] * (len(word_tok)-1))\n",
    "                    word2ids[word] = {\n",
    "                        'target_ids': word_tok,\n",
    "                        'label_id': label_id \n",
    "                    }\n",
    "                    text = text.replace(tagged_word, word)\n",
    "\n",
    "            tokenized = self.tokenizer(text, truncation=True, max_length=self.max_length, padding='max_length')\n",
    "            if 0 in tokenized['input_ids']:\n",
    "                tok_length = tokenized['input_ids'].index(0)\n",
    "            else:\n",
    "                tok_length = self.max_length\n",
    "            label_input = tokenized['input_ids'][:tok_length]\n",
    "            labels = self._gen_labels(label_input, word2ids)\n",
    "            labels.insert(0, -100)\n",
    "            pad = [-100] * (self.max_length - len(labels))\n",
    "            labels.extend(pad)\n",
    "\n",
    "            temp = {\n",
    "                'input_ids' : tokenized['input_ids'],\n",
    "                'attention_mask' : tokenized['attention_mask'],\n",
    "                'labels' : labels\n",
    "            }\n",
    "            \n",
    "            self.data.append(temp)\n",
    "        \n",
    "    def _gen_labels(self, input_ids, word2ids):\n",
    "        sequence = input_ids[1:-1]\n",
    "        labels = [0] * len(sequence)\n",
    "        \n",
    "        for v in word2ids.values():\n",
    "            target_ids = v['target_ids']\n",
    "            label_id = v['label_id']\n",
    "            \n",
    "            i=0\n",
    "            target_ids_length = len(target_ids)\n",
    "            \n",
    "            while i < len(sequence):\n",
    "                if sequence[i:i + target_ids_length] == target_ids:\n",
    "                    labels[i:i + target_ids_length] = label_id\n",
    "                    i = i + target_ids_length\n",
    "                else:\n",
    "                    i += 1\n",
    "                    \n",
    "        return labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.data[idx]['input_ids'],\n",
    "            'attention_mask': self.data[idx]['attention_mask'],\n",
    "            'labels': self.data[idx]['labels'],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IOBE(IOBES):\n",
    "    allowed_prefix = Prefix.I | Prefix.O | Prefix.B | Prefix.E\n",
    "    start_patterns = {\n",
    "        (Prefix.ANY, Prefix.B, Tag.ANY),\n",
    "        (Prefix.ANY, Prefix.S, Tag.ANY)\n",
    "    }\n",
    "    inside_patterns = {\n",
    "        (Prefix.B, Prefix.I, Tag.SAME),\n",
    "        (Prefix.B, Prefix.E, Tag.SAME),\n",
    "        (Prefix.I, Prefix.I, Tag.SAME),\n",
    "        (Prefix.I, Prefix.E, Tag.SAME)\n",
    "    }\n",
    "    end_patterns = {\n",
    "        (Prefix.S, Prefix.ANY, Tag.ANY),\n",
    "        (Prefix.E, Prefix.ANY, Tag.ANY),\n",
    "        (Prefix.B, Prefix.O, Tag.ANY),\n",
    "        (Prefix.B, Prefix.I, Tag.DIFF),\n",
    "        (Prefix.B, Prefix.B, Tag.ANY),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    #import pdb;pdb.set_trace()\n",
    "    predictions = predictions.flatten()\n",
    "    labels = labels.flatten()\n",
    "    npre = []\n",
    "    nlab = []\n",
    " \n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] != -100:\n",
    "            npre.append(predictions[i])\n",
    "            nlab.append(labels[i])\n",
    "    npre = torch.tensor(npre)\n",
    "    nlab = torch.tensor(nlab)\n",
    "    \n",
    "    label_indices = label_arr.copy()\n",
    "    npre = [label_indices[pred] for pred in npre]\n",
    "    nlab = [label_indices[label] for label in nlab]\n",
    "    del label_indices[label_indices.index(\"O\")]\n",
    "    entity_level_metrics = classification_report(\n",
    "        [nlab], [npre], digits=3,\n",
    "        suffix=False,\n",
    "        mode= 'strict', scheme=IOBE, \n",
    "        zero_division=True, output_dict=True\n",
    "    )\n",
    "\n",
    "    metrics = {}\n",
    "    \n",
    "    for key in entity_level_metrics.keys():\n",
    "        if len(key) == 3:\n",
    "            metrics[key+\"_f1\"] = entity_level_metrics[key]['f1-score']\n",
    "            metrics[key+\"_recall\"] = entity_level_metrics[key]['recall']\n",
    "            metrics[key+\"_precision\"] = entity_level_metrics[key]['precision']\n",
    "            \n",
    "        if key == 'macro avg':\n",
    "            metrics[\"entity_macro_f1\"] = entity_level_metrics['macro avg']['f1-score']\n",
    "            metrics[\"entity_macro_precision\"] = entity_level_metrics['macro avg']['precision']\n",
    "            metrics[\"entity_macro_recall\"] = entity_level_metrics['macro avg']['recall']\n",
    "            \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_train(config):\n",
    "      \n",
    "    model = ElectraForTokenClassification.from_pretrained(config['base_model_dir'], num_labels=3)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config['base_model_dir'])\n",
    "    train_file = pd.read_csv(config['train_file'])\n",
    "    valid_file = pd.read_csv(config['valid_file'], sep='\\t') \n",
    "\n",
    "    ent = config['new_entity']\n",
    "    \n",
    "    label2id = {\n",
    "        'O' : 0,\n",
    "        'B-'+ent : 1,\n",
    "        'I-'+ent : 2\n",
    "    }\n",
    "\n",
    "    train_data = NERDataset(train_file, tokenizer=tokenizer, max_length=300, label2id=label2id)\n",
    "    valid_data = NERDataset(valid_file, tokenizer=tokenizer, max_length=300, label2id=label2id)\n",
    "    id2label = {label2id[label] : label for label in label2id.keys()}\n",
    "    \n",
    "    model.config.label2id = label2id\n",
    "    model.config.id2label = id2label\n",
    "\n",
    "    global label_arr\n",
    "    \n",
    "    label_arr = []\n",
    "    for v in id2label.values():\n",
    "        label_arr.append(v)\n",
    "\n",
    "    data_collator = DefaultDataCollator()\n",
    "    device = torch.device(\"cuda\")\n",
    "    model.to(device)\n",
    "     \n",
    "    training_args = TrainingArguments(\n",
    "\n",
    "        output_dir=config['output_dir'],\n",
    "        do_eval = True,\n",
    "        learning_rate=config['learning_rate'],\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=config['train_epoch'],\n",
    "        weight_decay=0.1,\n",
    "        save_strategy = 'epoch',\n",
    "        logging_strategy = 'epoch',\n",
    "        evaluation_strategy = 'epoch',\n",
    "        load_best_model_at_end = True,\n",
    "        label_names = ['labels'],\n",
    "        metric_for_best_model = 'entity_macro_f1',\n",
    "        warmup_ratio = 0.05,\n",
    "        no_cuda = False\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset = valid_data,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer = tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(config['output_dir'] + '/final')\n",
    "    \n",
    "    for f_name in os.listdir(config['output_dir']):\n",
    "        if f_name.startswith('checkpoint'):\n",
    "            for f in os.listdir(config['output_dir']+'/'+f_name):\n",
    "                os.remove(config['output_dir']+'/'+f_name+'/'+f)\n",
    "            os.rmdir(config['output_dir']+'/'+f_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from transformers import TrainerCallback\n",
    "\n",
    "# class customCallBack(TrainerCallback):\n",
    "#     def on_epoch_end(self, args, state, control, model, **kwargs):\n",
    "#         import pdb;pdb.set_trace()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cl_train(config):\n",
    "    \n",
    "    teacher = ElectraForTokenClassification.from_pretrained(config['teacher_dir'])\n",
    "    student = ex.extendNER.from_pretrained(config['teacher_dir'])\n",
    "    labels = student.num_labels\n",
    "    student.config.id2label[labels-2] = 'B-' + config['new_entity']\n",
    "    student.config.id2label[labels-1] = 'I-' + config['new_entity']\n",
    "    student.config.label2id['B-'+config['new_entity']] = labels-2\n",
    "    student.config.label2id['I-'+config['new_entity']] = labels-1\n",
    "    student.ce = config['ce']\n",
    "    student.kd = config['kd']\n",
    "    student.T = config['T']\n",
    "    torch.manual_seed(42)\n",
    "    student.classifier = torch.nn.Linear(768, labels)\n",
    "    \n",
    "    global label_arr\n",
    "    \n",
    "    label_arr = []\n",
    "    for v in student.config.id2label.values():\n",
    "        label_arr.append(v)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(config['teacher_dir'])\n",
    "    \n",
    "    train_file = pd.read_csv(config['train_file'])\n",
    "    valid_file = pd.read_csv(config['valid_file'], sep='\\t')\n",
    "    \n",
    "    label2id = student.config.label2id\n",
    "    \n",
    "    train_dataset = NERDataset(train_file, tokenizer=tokenizer, max_length=300, label2id=label2id)\n",
    "    valid_dataset = NERDataset(valid_file, tokenizer=tokenizer, max_length=300, label2id=label2id)\n",
    "    \n",
    "    temp_train = []\n",
    "    temp_valid = []\n",
    "    for e in train_dataset:\n",
    "        temp_train.append(e)\n",
    "        \n",
    "    for e in valid_dataset:\n",
    "        temp_valid.append(e)   \n",
    "        \n",
    "    teacher_input = pd.DataFrame(temp_train)\n",
    "    student_input = teacher_input.copy()\n",
    "    teacher_input = teacher_input.drop(columns=['labels'])\n",
    "    teacher_input = Dataset.from_pandas(teacher_input)\n",
    "    \n",
    "    data_collator = DefaultDataCollator()\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=teacher,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "\n",
    "    prediction = trainer.predict(teacher_input)\n",
    "    \n",
    "    soft_label = prediction[0].tolist()\n",
    "    student_input['soft_labels'] = soft_label\n",
    "    student_input = student_input.rename(columns={'labels' :'hard_labels'})\n",
    "\n",
    "    valid_input = pd.DataFrame(temp_valid)\n",
    "\n",
    "    student_input = Dataset.from_pandas(student_input)\n",
    "    valid_input = Dataset.from_pandas(valid_input)\n",
    "    \n",
    "    data_collator = DefaultDataCollator()\n",
    "    \n",
    "    device = torch.device(\"cuda\")\n",
    "    student.to(device)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=config['output_dir'],\n",
    "        do_eval = True,\n",
    "        learning_rate=config['learning_rate'],\n",
    "        per_device_train_batch_size=config['batch'],\n",
    "        per_device_eval_batch_size=config['batch'],\n",
    "        num_train_epochs=config['train_epoch'],\n",
    "        weight_decay=0.1,\n",
    "        save_strategy = 'epoch',\n",
    "        logging_strategy = 'epoch',\n",
    "        evaluation_strategy = 'epoch',\n",
    "        load_best_model_at_end = True,\n",
    "        label_names = ['labels'],\n",
    "        metric_for_best_model = 'entity_macro_f1',\n",
    "        warmup_ratio = 0.05,\n",
    "        no_cuda = False,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=student,\n",
    "        args=training_args,\n",
    "        train_dataset=student_input,\n",
    "        eval_dataset = valid_input,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer = tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(config['output_dir'] + '/final')\n",
    "    \n",
    "    for f_name in os.listdir(config['output_dir']):\n",
    "        if f_name.startswith('checkpoint'):\n",
    "            for f in os.listdir(config['output_dir']+'/'+f_name):\n",
    "                os.remove(config['output_dir']+'/'+f_name+'/'+f)\n",
    "            os.rmdir(config['output_dir']+'/'+f_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(config):\n",
    "    \n",
    "    model = ElectraForTokenClassification.from_pretrained(config['model'])\n",
    "    \n",
    "    global label_arr\n",
    "    \n",
    "    label_arr = []\n",
    "    for v in model.config.id2label.values():\n",
    "        label_arr.append(v)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(config['model'])\n",
    "    \n",
    "    test_file = pd.read_csv(config['test_file'], sep='\\t')\n",
    "    label2id = model.config.label2id\n",
    "    \n",
    "    test_dataset = NERDataset(test_file, tokenizer=tokenizer, max_length=300, label2id=label2id)\n",
    "    \n",
    "    device = torch.device(\"cuda\")\n",
    "    model.to(device)\n",
    "    \n",
    "    data_collator = DefaultDataCollator()\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=config['model'],\n",
    "        per_device_eval_batch_size=32,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        args=training_args,\n",
    "        model=model,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    \n",
    "    metrics = trainer.evaluate(test_dataset)\n",
    "    trainer.save_metrics(split='test', metrics=metrics)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraForTokenClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/home/jjs970612/anaconda3/envs/temp/lib/python3.7/site-packages/transformers/optimization.py:415: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1310' max='1310' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1310/1310 10:59, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Org F1</th>\n",
       "      <th>Org Recall</th>\n",
       "      <th>Org Precision</th>\n",
       "      <th>Entity Macro F1</th>\n",
       "      <th>Entity Macro Precision</th>\n",
       "      <th>Entity Macro Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.278400</td>\n",
       "      <td>0.038075</td>\n",
       "      <td>0.825150</td>\n",
       "      <td>0.876590</td>\n",
       "      <td>0.779412</td>\n",
       "      <td>0.825150</td>\n",
       "      <td>0.779412</td>\n",
       "      <td>0.876590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.034100</td>\n",
       "      <td>0.033878</td>\n",
       "      <td>0.849349</td>\n",
       "      <td>0.871501</td>\n",
       "      <td>0.828295</td>\n",
       "      <td>0.849349</td>\n",
       "      <td>0.828295</td>\n",
       "      <td>0.871501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.016300</td>\n",
       "      <td>0.038848</td>\n",
       "      <td>0.859077</td>\n",
       "      <td>0.888041</td>\n",
       "      <td>0.831943</td>\n",
       "      <td>0.859077</td>\n",
       "      <td>0.831943</td>\n",
       "      <td>0.888041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.047528</td>\n",
       "      <td>0.840093</td>\n",
       "      <td>0.922392</td>\n",
       "      <td>0.771277</td>\n",
       "      <td>0.840093</td>\n",
       "      <td>0.771277</td>\n",
       "      <td>0.922392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>0.048643</td>\n",
       "      <td>0.850589</td>\n",
       "      <td>0.872774</td>\n",
       "      <td>0.829504</td>\n",
       "      <td>0.850589</td>\n",
       "      <td>0.829504</td>\n",
       "      <td>0.872774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.052179</td>\n",
       "      <td>0.843529</td>\n",
       "      <td>0.912214</td>\n",
       "      <td>0.784464</td>\n",
       "      <td>0.843529</td>\n",
       "      <td>0.784464</td>\n",
       "      <td>0.912214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.055098</td>\n",
       "      <td>0.850602</td>\n",
       "      <td>0.898219</td>\n",
       "      <td>0.807780</td>\n",
       "      <td>0.850602</td>\n",
       "      <td>0.807780</td>\n",
       "      <td>0.898219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.056828</td>\n",
       "      <td>0.852816</td>\n",
       "      <td>0.895674</td>\n",
       "      <td>0.813873</td>\n",
       "      <td>0.852816</td>\n",
       "      <td>0.813873</td>\n",
       "      <td>0.895674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.058852</td>\n",
       "      <td>0.853865</td>\n",
       "      <td>0.899491</td>\n",
       "      <td>0.812644</td>\n",
       "      <td>0.853865</td>\n",
       "      <td>0.812644</td>\n",
       "      <td>0.899491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.059077</td>\n",
       "      <td>0.853556</td>\n",
       "      <td>0.908397</td>\n",
       "      <td>0.804961</td>\n",
       "      <td>0.853556</td>\n",
       "      <td>0.804961</td>\n",
       "      <td>0.908397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [95/95 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./ce_full/perm_1/step1/final were not used when initializing extendNER: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing extendNER from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing extendNER from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/home/jjs970612/anaconda3/envs/temp/lib/python3.7/site-packages/transformers/optimization.py:415: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3915' max='5240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3915/5240 18:44 < 06:20, 3.48 it/s, Epoch 14.94/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Org F1</th>\n",
       "      <th>Org Recall</th>\n",
       "      <th>Org Precision</th>\n",
       "      <th>Per F1</th>\n",
       "      <th>Per Recall</th>\n",
       "      <th>Per Precision</th>\n",
       "      <th>Entity Macro F1</th>\n",
       "      <th>Entity Macro Precision</th>\n",
       "      <th>Entity Macro Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.579300</td>\n",
       "      <td>0.480743</td>\n",
       "      <td>0.061200</td>\n",
       "      <td>0.031807</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>0.152903</td>\n",
       "      <td>0.918495</td>\n",
       "      <td>0.083393</td>\n",
       "      <td>0.107051</td>\n",
       "      <td>0.444922</td>\n",
       "      <td>0.475151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.042700</td>\n",
       "      <td>0.616746</td>\n",
       "      <td>0.002535</td>\n",
       "      <td>0.001272</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.133785</td>\n",
       "      <td>0.835423</td>\n",
       "      <td>0.072715</td>\n",
       "      <td>0.068160</td>\n",
       "      <td>0.203024</td>\n",
       "      <td>0.418348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.018900</td>\n",
       "      <td>0.534238</td>\n",
       "      <td>0.671569</td>\n",
       "      <td>0.522901</td>\n",
       "      <td>0.938356</td>\n",
       "      <td>0.181122</td>\n",
       "      <td>0.926332</td>\n",
       "      <td>0.100374</td>\n",
       "      <td>0.426345</td>\n",
       "      <td>0.519365</td>\n",
       "      <td>0.724617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>0.604311</td>\n",
       "      <td>0.299465</td>\n",
       "      <td>0.178117</td>\n",
       "      <td>0.939597</td>\n",
       "      <td>0.227404</td>\n",
       "      <td>0.802508</td>\n",
       "      <td>0.132471</td>\n",
       "      <td>0.263435</td>\n",
       "      <td>0.536034</td>\n",
       "      <td>0.490312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.009900</td>\n",
       "      <td>0.795538</td>\n",
       "      <td>0.737221</td>\n",
       "      <td>0.651399</td>\n",
       "      <td>0.849088</td>\n",
       "      <td>0.103017</td>\n",
       "      <td>0.703762</td>\n",
       "      <td>0.055576</td>\n",
       "      <td>0.420119</td>\n",
       "      <td>0.452332</td>\n",
       "      <td>0.677581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.008200</td>\n",
       "      <td>0.984511</td>\n",
       "      <td>0.263904</td>\n",
       "      <td>0.153944</td>\n",
       "      <td>0.923664</td>\n",
       "      <td>0.116969</td>\n",
       "      <td>0.778997</td>\n",
       "      <td>0.063232</td>\n",
       "      <td>0.190436</td>\n",
       "      <td>0.493448</td>\n",
       "      <td>0.466470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.866057</td>\n",
       "      <td>0.597222</td>\n",
       "      <td>0.437659</td>\n",
       "      <td>0.939891</td>\n",
       "      <td>0.124296</td>\n",
       "      <td>0.830721</td>\n",
       "      <td>0.067174</td>\n",
       "      <td>0.360759</td>\n",
       "      <td>0.503532</td>\n",
       "      <td>0.634190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.840348</td>\n",
       "      <td>0.638544</td>\n",
       "      <td>0.491094</td>\n",
       "      <td>0.912530</td>\n",
       "      <td>0.119965</td>\n",
       "      <td>0.849530</td>\n",
       "      <td>0.064539</td>\n",
       "      <td>0.379254</td>\n",
       "      <td>0.488534</td>\n",
       "      <td>0.670312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.805771</td>\n",
       "      <td>0.616554</td>\n",
       "      <td>0.464377</td>\n",
       "      <td>0.917085</td>\n",
       "      <td>0.143754</td>\n",
       "      <td>0.843260</td>\n",
       "      <td>0.078575</td>\n",
       "      <td>0.380154</td>\n",
       "      <td>0.497830</td>\n",
       "      <td>0.653818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.816168</td>\n",
       "      <td>0.623006</td>\n",
       "      <td>0.472010</td>\n",
       "      <td>0.916049</td>\n",
       "      <td>0.145218</td>\n",
       "      <td>0.840125</td>\n",
       "      <td>0.079478</td>\n",
       "      <td>0.384112</td>\n",
       "      <td>0.497764</td>\n",
       "      <td>0.656068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.788598</td>\n",
       "      <td>0.676752</td>\n",
       "      <td>0.540712</td>\n",
       "      <td>0.904255</td>\n",
       "      <td>0.157436</td>\n",
       "      <td>0.835423</td>\n",
       "      <td>0.086907</td>\n",
       "      <td>0.417094</td>\n",
       "      <td>0.495581</td>\n",
       "      <td>0.688068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.814693</td>\n",
       "      <td>0.571921</td>\n",
       "      <td>0.422392</td>\n",
       "      <td>0.885333</td>\n",
       "      <td>0.144269</td>\n",
       "      <td>0.835423</td>\n",
       "      <td>0.078951</td>\n",
       "      <td>0.358095</td>\n",
       "      <td>0.482142</td>\n",
       "      <td>0.628908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.790764</td>\n",
       "      <td>0.536542</td>\n",
       "      <td>0.382952</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.150228</td>\n",
       "      <td>0.851097</td>\n",
       "      <td>0.082385</td>\n",
       "      <td>0.343385</td>\n",
       "      <td>0.489109</td>\n",
       "      <td>0.617024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.791383</td>\n",
       "      <td>0.572174</td>\n",
       "      <td>0.418575</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.152613</td>\n",
       "      <td>0.835423</td>\n",
       "      <td>0.083977</td>\n",
       "      <td>0.362393</td>\n",
       "      <td>0.493911</td>\n",
       "      <td>0.626999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18110/978663508.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 }\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0mcl_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcl_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             test_config = {\n",
      "\u001b[0;32m/tmp/ipykernel_18110/4060640430.py\u001b[0m in \u001b[0;36mcl_train\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     94\u001b[0m     )\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output_dir'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/final'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/temp/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1647\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1648\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1649\u001b[0;31m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1650\u001b[0m         )\n\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/temp/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1937\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1938\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1940\u001b[0m                 if (\n",
      "\u001b[0;32m~/anaconda3/envs/temp/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2768\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2769\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2770\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2772\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/temp/lib/python3.7/site-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1819\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1820\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1821\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1823\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0munscale_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/temp/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[1;32m    488\u001b[0m         torch.autograd.backward(\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         )\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/temp/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "if(__name__==\"__main__\"):\n",
    "    \n",
    "    directory = './ce_full'\n",
    "\n",
    "    for per_num_int in range(1,7):\n",
    "        per_num = str(per_num_int)\n",
    "        permutation_dir = \"train_data/perm_\" + per_num\n",
    "\n",
    "        data_file_dict = {\n",
    "            \"train\" : [],\n",
    "            \"valid\" : []\n",
    "\n",
    "        }\n",
    "\n",
    "    #     permutations = [['ORG', 'PER', 'CVL', 'DAT', 'LOC', 'QNT'],\n",
    "    #                    ['DAT', 'QNT', 'PER', 'LOC', 'ORG', 'CVL'],\n",
    "    #                    ['CVL', 'LOC', 'ORG', 'QNT', 'DAT', 'PER'],\n",
    "    #                    ['QNT', 'ORG', 'DAT', 'PER', 'CVL', 'LOC'],\n",
    "    #                    ['LOC', 'CVL', 'QNT', 'ORG', 'PER', 'DAT'],\n",
    "    #                    ['PER', 'DAT', 'LOC', 'CVL', 'QNT', 'ORG']]\n",
    "\n",
    "        for i in range(6):\n",
    "            for f_name in os.listdir(permutation_dir + '/'):\n",
    "                if f_name.startswith('d'+str(i)):\n",
    "                    data_file_dict['train'].append(f_name)\n",
    "                    break\n",
    "\n",
    "            for f_name in os.listdir('test_data/perm' + str(per_num)):\n",
    "                if f_name.startswith('eval_'+str(i+1)):\n",
    "                    data_file_dict['valid'].append(f_name)\n",
    "                    break\n",
    "\n",
    "        for i in range(6):\n",
    "\n",
    "            new_ent = data_file_dict['train'][i].split('_')[1][:3]\n",
    "            if i == 0:\n",
    "\n",
    "                base_config = {\n",
    "                    'base_model_dir' : 'monologg/koelectra-base-v3-discriminator',\n",
    "                    'train_file' : permutation_dir + '/' + data_file_dict['train'][0],\n",
    "                    'valid_file' : 'test_data/perm' + str(per_num) + '/' + data_file_dict['valid'][0],\n",
    "                    'output_dir' : directory + '/perm_' + per_num + '/step1/',\n",
    "                    'train_epoch' : 10,\n",
    "                    'learning_rate' : 5e-05,\n",
    "                    'new_entity' : new_ent\n",
    "                }\n",
    "\n",
    "                base_train(base_config)\n",
    "\n",
    "            else:\n",
    "\n",
    "                cl_config = {\n",
    "                    'teacher_dir' : directory + '/perm_' + per_num + '/step' + str(i) + '/final',\n",
    "                    'new_entity' : new_ent,\n",
    "                    'train_file' : permutation_dir + '/' + data_file_dict['train'][i],\n",
    "                    'valid_file' : 'test_data/perm' + str(per_num) + '/' + data_file_dict['valid'][i],\n",
    "                    'output_dir' : directory + '/perm_' + per_num + '/step' + str(i+1) + '/',\n",
    "                    'train_epoch' : 20,\n",
    "                    'learning_rate' : 5e-05,\n",
    "                    'batch' : 16,\n",
    "                    'ce' : 1,\n",
    "                    'kd' : 1,\n",
    "                    'T' : 2\n",
    "                }\n",
    "\n",
    "                cl_train(cl_config)\n",
    "\n",
    "            test_config = {\n",
    "                'model': directory + '/perm_' + per_num + '/step' + str(i+1) + '/final',\n",
    "                'test_file': 'test_data/perm' + str(per_num) + '/' + data_file_dict['valid'][i]\n",
    "            }    \n",
    "\n",
    "            test(test_config)\n",
    "\n",
    "    # folder = directory\n",
    "    # for perm in os.listdir(folder):\n",
    "    #     for step in os.listdir(folder + '/'+perm):\n",
    "    #         for files in os.listdir(folder+'/'+perm+'/'+step+'/final'):\n",
    "    #             if files.startswith('test'):\n",
    "    #                 pass\n",
    "    #             else:\n",
    "    #                 if os.path.isdir(folder+'/'+perm+'/'+step+'/final/'+files):\n",
    "    #                     shutil.rmtree(folder+'/'+perm+'/'+step+'/final/'+files)\n",
    "    #                 else:\n",
    "    #                     os.remove(folder+'/'+perm+'/'+step+'/final/'+files) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "junseo",
   "language": "python",
   "name": "temp_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
